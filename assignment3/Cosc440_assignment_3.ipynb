{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gY_Ma-ke0eY"
      },
      "source": [
        "# COSC440 Assignment 3: Building a Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xeaFoKZe0ea"
      },
      "source": [
        "In the first part of the assignment, we will be introducing and getting familiar with the concept of a convolutional autoencoder. We will use this to denoise images from the CIFAR10 dataset. In the second part we will attempt to apply to this model to explore unsupervised classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HJoXp45e0eb"
      },
      "source": [
        "## Introduction\n",
        "Autoencoders are really cool. They use this architecture of encoder + decoder to learn data representation in an unsupervised manner. Sounds complicated? Let's break it down.\n",
        "\n",
        "\n",
        "![Autoencoders](https://drive.google.com/uc?export=view&id=18iU9_yZfqs2QlTPbaibzkMtdIekLuPj2)\n",
        "\n",
        "As we can see, the autoencoder is divided roughly in two parts: the encoder that encodes the input to a \"code\", or internal representation, that is reduced in dimensions, and the decoder that decodes the \"code\" to an output that is the same dimensions as the input.\n",
        "\n",
        "The autoencoder tries to learn a function h(x)≈x. In other words, it is trying to learn an approximation to the identity function, so as to output x'  that is similar to x. This may sound like a pretty trivial task, why not just use the identity function? Well, it's impossible to find a perfect representation of the identity function since we reduced dimensionality of the input into the code during the encoding part, and try to \"reconstruct\" the original image using only the dimensionally reduced code. E.x. Given an input of a 10x10 image, and a hidden size of 50, the encoder is forced to learn a compressed representation of the image (from 100 units to 50 units). This encoding is almost always lossy, but the point is that we train the encoder to recognize important structures in the input and throw away the unimportant noise.\n",
        "\n",
        "### In this part, we will be building a denoising autoencoder. The input of the autoencoder will be a noisy version of our training data and we will train the model to predict the noise free version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAcauSgEe0ec"
      },
      "source": [
        "## Deconvolution (Transposed-Convolution)\n",
        "Conceptually, deconvolution reverses the effects of convolution. Convolution with a stride of larger than 1 reduces the size of the input, while deconvolution can be described as a \"fractional stride\" convolution, it upscales the size of the input. Here's a visualisation.\n",
        "\n",
        "#### Convolution with stride = 2 *going from bigger to smaller*:\n",
        "![convolution.gif](https://drive.google.com/uc?export=view&id=1TjSgMinNQ629TrdEmcVjKuwWZ9MGMfg7)\n",
        "\n",
        "#### Deconvolution (convolution with fractional stride) *going from smaller to bigger*:\n",
        "![deconvolution.gif](https://drive.google.com/uc?export=view&id=1opzIaiShA5_nLT5Y5Kh_GqPT9IlVeQF5)\n",
        "\n",
        "As we can see, deconvolution upscales the input. This is important for our decoder as its job is to upscale the encoded and compressed internal representation to its original size.\n",
        "Concretely, this is how we implement deconvolution in tensorflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEvAON8ye0ef"
      },
      "source": [
        "``` Python\n",
        "tf.nn.conv2d_transpose(\n",
        "    value, # input tensor: A 4-D Tensor of type float and shape [batch, height, width, in_channels]\n",
        "    filter, # filter tensor: A 4-D Tensor with shape [height, width, output_channels, in_channels]\n",
        "    output_shape, # A 1-D Tensor representing the output shape of the deconvolution op.\n",
        "    strides, # A list of ints. The stride of the sliding window for each dimension of the input tensor.\n",
        "    padding='SAME'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9K0ELkhe0fF"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Enabling the GPU:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "\n",
        "**Note: We are forcing installation of TensorFlow version 2.15.0.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oZGwvU5UsYV",
        "outputId": "c1dc7f4d-fcc8-4719-d229-6e3f0c39cc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHX7FYAJ2RtZ",
        "outputId": "b5fa502c-d0ac-4036-f6e9-11fda71af64f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.0\n",
            "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# # This makes sure that we are using Tensorflow 2\n",
        "# try:\n",
        "#   # Note: %tensorflow_version only exists in Google Colab\n",
        "#   %tensorflow_version 2.x\n",
        "# except Exception:\n",
        "#     pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(1337)  # setting seed for later part of assignment. DO NOT CHANGE\n",
        "\n",
        "print(tf.__version__)\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9fO-Ccn3wRF"
      },
      "source": [
        "If set up correctly, you should see GPU Available: [PhysicalDevice]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHQpPy-tgl3I"
      },
      "source": [
        "### Noise Functions\n",
        "\n",
        "In order to test our autoencoder, we need to add some noise to our CIFAR10 data points. The goal for the autoencoder is to learn the important underlying features (the objects) of the data and to ignore the extra noise.\n",
        "\n",
        "Implement the below different noise functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-qxm7NGloBT",
        "outputId": "38835cb5-e3e9-405a-cbdf-375bbdcc2e0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[0.3, 0.1],\n",
            "       [0.2, 0. ]], dtype=float32)>\n",
            "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[0.1 , 1.  ],\n",
            "       [0.  , 0.33]], dtype=float32)>\n",
            "Noise functions look good!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This function adds no noise, we use this as a way of comparing the\n",
        "autoencoder when there's no input noise\n",
        "\"\"\"\n",
        "def no_noise(x):\n",
        "  # TODO\n",
        "  return tf.Variable(x)\n",
        "\n",
        "\"\"\"\n",
        "This function should add a random uniform tensor of the shape of x between\n",
        "-0.3 to 0.3 to x.\n",
        "\n",
        "It should then \"clip\" x to between 0 and 1 (hint: check out tf.clip_by_value)\n",
        "\"\"\"\n",
        "def random_noise(x):\n",
        "  # TODO\n",
        "  x = tf.Variable(x)\n",
        "  print(x)\n",
        "  noise = tf.Variable(tf.random.uniform(x.shape,\n",
        "                minval=-0.3,\n",
        "                maxval=0.3,\n",
        "                dtype=tf.float32),\n",
        "                name = \"noise\")\n",
        "  return tf.clip_by_value(x + noise, 0, 1)\n",
        "\n",
        "\"\"\"\n",
        "This function should multiply a random uniform tensor of the shape of x between\n",
        "0 to 2.0 to x.\n",
        "\n",
        "It should then \"clip\" x to between 0 and 1 (hint: check out tf.clip_by_value)\n",
        "\"\"\"\n",
        "def random_scale(x):\n",
        "  # TODO\n",
        "  x = tf.Variable(x)\n",
        "  noise = tf.Variable(tf.random.uniform(x.shape,\n",
        "                minval=0,\n",
        "                maxval=2.0,\n",
        "                dtype=tf.float32),\n",
        "                name = \"noise\")\n",
        "  return tf.clip_by_value(x + noise, 0, 1)\n",
        "\n",
        "# some \"unit tests\"\n",
        "x = [[0.3,0.1],[0.2,0]]\n",
        "y = [[0.1,1],[0,0.33]]\n",
        "result_1 = random_noise(x)\n",
        "result_2 = random_scale(x)\n",
        "result_3 = random_noise(y)\n",
        "result_4 = random_scale(y)\n",
        "\n",
        "for res in [result_1, result_2, result_3, result_4]:\n",
        "  assert(res.shape == (2,2))\n",
        "  assert(res.dtype == tf.float32)\n",
        "  assert(np.max(np.array(res)) <= 1.0)\n",
        "  assert(np.min(np.array(res)) >= 0.0)\n",
        "\n",
        "print(\"Noise functions look good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWuS-lvYe0fU"
      },
      "source": [
        "## Architecture Definition\n",
        "We want to construct the following architecture:\n",
        "\n",
        "(Due to the limitations of the rendering engine I'm using, there is a small inaccuracy: the connections in the decoder *{the red prisms}* should be upscaling, not downscaling like in the encoder. - original creator of lab)\n",
        "\n",
        "Shapes:\n",
        "\n",
        "Layer | Shape of Output\n",
        "--- | ---\n",
        "Input | (batch_size, 32, 32, 3)\n",
        "encoder_conv_1    | (batch_size, 16, 16, 8)\n",
        "encoder_conv_2    | (batch_size, 8, 8, 16)  \n",
        "encoder_conv_3    | (batch_size, 4, 4, 32)  \n",
        "decoder_deconv_1  | (batch_size, 8, 8, 16)  \n",
        "decoder_deconv_2  | (batch_size, 16, 16, 8)\n",
        "decoder_deconv_3  | (batch_size, 32, 32, 3)\n",
        "\n",
        "![Architecture](https://drive.usercontent.google.com/download?id=1-jZFte-UE-L-lFiTHlAhgFyDDYPXRVvT&export=view)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp9a4duA4_Z-"
      },
      "source": [
        "### **Encoder**\n",
        "Let's first build the encoding portion of our autoencoder. We're going to directly use Keras layers to help us out by subclassing the Layer class. See this for more information on subclassing Layers: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
        "\n",
        "This should have 3 convolution layers. The layers should be initialized with random normal with standard deviation of 0.1. Also, please use an activation layer of leaky_relu with alpha=0.2. Use a kernel size of 3 and padding of same. Think about what the filter count and stride should be by looking at the output shapes of the architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjlwlP5R5EZY"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "       super(Encoder, self).__init__()\n",
        "       self.encoder_conv_1 =\n",
        "       self.encoder_conv_2 =\n",
        "       self.encoder_conv_3 =\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, images):\n",
        "        return ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVQgB6sg9cwP"
      },
      "source": [
        "### **Decoder**\n",
        "Now let's build the decoder portion of our autoencoder.\n",
        "\n",
        "This should have 3 deconvolution layers. For the first deconvolution layer please use tf.nn.conv2d_transpose to implement the deconvolution. The other two may be keras layers (Conv2DTranspose). Use the same kernel / activation /initialization as the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbVcfiq29hIs"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder_deconv_1 =\n",
        "        self.decoder_deconv_2 =\n",
        "        self.decoder_deconv_3 =\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, encoder_output):\n",
        "        return ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XNWonQwEX3S"
      },
      "source": [
        "Now let's combine everything to create our autoencoder.\n",
        "\n",
        "Fill in the call and loss_function.\n",
        "\n",
        "Our loss equation is:\n",
        "$$\\sum (y - x)^2$$\n",
        "\n",
        "\n",
        "y: the encoded image\n",
        "\n",
        "x: the original corrupted image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtD2UkkLEcIF"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, images):\n",
        "       return ?\n",
        "\n",
        "    @tf.function\n",
        "    def loss_function(self, encoded, originals):\n",
        "      encoded = tf.dtypes.cast(encoded, tf.float32)\n",
        "      originals = tf.dtypes.cast(originals, tf.float32)\n",
        "      return ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8PdssiPqQi8"
      },
      "source": [
        "### Train Loop\n",
        "Next, please fill in the train loop for training.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUUT4loWtllD"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, images, noise_function):\n",
        "  corrupted = noise_function(images)\n",
        "  uncorrupted = images\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOaCajFzqZQO"
      },
      "source": [
        "### Total Loss\n",
        "This function that calculates the total loss over the entire data set,\n",
        "we use this to track the loss over each epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxVP3tDatlvQ"
      },
      "outputs": [],
      "source": [
        "def total_loss(model, images, noise_function):\n",
        "  corrupted = noise_function(images)\n",
        "  uncorrupted = images\n",
        "  predictions = model(corrupted)\n",
        "  sum_loss = model.loss_function(predictions, uncorrupted)\n",
        "  return sum_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYgBuoPAmvRa"
      },
      "source": [
        "## Test your autoencoder\n",
        "\n",
        "Run the following code to run your autoencoder. You should see it print out images and the total loss of the autoencoder each epoch.\n",
        "\n",
        "Be aware that the loss generally should go down each epoch, but might not always. Use your judgement and check with the TA after this runs.\n",
        "\n",
        "It probably will take around 5 minutes (make sure you are using a GPU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPBJIxfsq1fG"
      },
      "outputs": [],
      "source": [
        "# This code is helper code to plot the cifar10 images so you\n",
        "# can see your autoencoder in action!\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_examples = 10\n",
        "batch_size = 100\n",
        "n_epochs = 25\n",
        "\n",
        "\n",
        "(train_images, _), (test_images, _) = tf.keras.datasets.cifar10.load_data()\n",
        "test_images = test_images / 255\n",
        "example_images = test_images[:n_examples]\n",
        "train_images = train_images / 255\n",
        "\n",
        "def showImages(model, noise_function):\n",
        "  corrupted = noise_function(example_images)\n",
        "  recon = tf.clip_by_value(model(corrupted), 0, 1)\n",
        "\n",
        "  fig, axs = plt.subplots(2, n_examples, figsize=(10, 2))\n",
        "  for example_i in range(n_examples):\n",
        "      axs[0][example_i].imshow(corrupted[example_i])\n",
        "      axs[1][example_i].imshow(recon[example_i])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Runs the autoencoder\n",
        "# We'll just be testing the random_noise function for training\n",
        "for noise_function in [random_noise]:\n",
        "  print(\"Showing autoencoder for noise function: {0}\".format(noise_function))\n",
        "  model = AutoEncoder()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "  for i in range(n_epochs):\n",
        "    for j in range(0, len(train_images), batch_size):\n",
        "      train(model, optimizer, train_images[j:j+batch_size], noise_function)\n",
        "\n",
        "    print(\"Epoch: \", i)\n",
        "    sum_loss = total_loss(model, test_images, noise_function)\n",
        "    print(\"Total Loss: {0}\".format(sum_loss))\n",
        "    showImages(model, noise_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sBNgo9Ze0fy"
      },
      "source": [
        "## Checkpoint!\n",
        "\n",
        "Check to see if your denoiser works! If it works, you should see the autoencoder images remove noise from the original inputs. The top row for each epoch is the input image and the bottom is the autoencoded image.\n",
        "\n",
        "If you wish to you can check out how the different noise functions are handled by the autoencoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6dyRrpWe0fz"
      },
      "source": [
        "## Visualizing the Data Representation\n",
        "\n",
        "We can now visualize where CIFAR10 data points are positioned in the latent space. Run the visualization code below to open Tensorboard. On the bottom right you can switch visualiztion to T-SNE or PCA for a 3D representation.\n",
        "\n",
        "Note: You may have to run the cell twice for it to actually render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XYQAxZt048Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "NUM_SAMPLES = 1000\n",
        "\n",
        "(train_images, train_labels),(test_images,test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "test_images = test_images / 255\n",
        "train_images = train_images / 255\n",
        "\n",
        "z_test = model.encoder(tf.reshape(test_images, [-1, 32, 32, 3]))\n",
        "z_test = np.reshape(z_test, [len(test_images), -1])\n",
        "\n",
        "z_train = model.encoder(tf.reshape(train_images, [-1, 32, 32, 3]))\n",
        "z_train = np.reshape(z_train, [len(train_images), -1])\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "log_dir = \"tensorboard/\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "total_images = len(test_images)\n",
        "\n",
        "z_test_tensor = torch.tensor(z_test[:total_images])\n",
        "test_labels_tensor = [int(test_labels[i]) for i in range(total_images)]\n",
        "test_images_tensor = torch.tensor(test_images[:total_images]).permute(0, 3, 1, 2)\n",
        "\n",
        "writer.add_embedding(z_test_tensor, metadata=test_labels_tensor, label_img=test_images_tensor)\n",
        "\n",
        "%tensorboard --logdir {log_dir}\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQgDnxHCW4Md"
      },
      "source": [
        "Now that we have a trained version of the auto-encoder we want to take the output of just the encoder and perform k-mean clustering on it. Fill out the code below and perform k-mean clustering on the latent space of the training examples.\n",
        "\n",
        "Note: Running k-means should take about 2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Aq7gMxQX0b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 10\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10, max_iter=1000)\n",
        "encoder = model.encoder\n",
        "# TODO create latent space embeddings of training images\n",
        "latent_representations = ?\n",
        "assert(tf.is_tensor(latent_representations))\n",
        "assert(latent_representations.shape[0] == 50000)\n",
        "assert(latent_representations.shape[0] == 512)\n",
        "print('latent_representations is the right shape and type')\n",
        "\n",
        "# TODO perform kmeans clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0q18pevslGP"
      },
      "source": [
        "Lets check our class seperations now. In the next cell, group the test images by class, calculate their latent space embeddings, and run kmeans.predict on them all.\n",
        "\n",
        "Use this to calculate the mode of the predictions for each of the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MH4BDj5t_dl"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "for i in range(NUM_CLASSES):\n",
        "  test_filter = test_labels == i\n",
        "  test_images_class_i = test_images[test_filter[:, 0]]\n",
        "  test_labels_class_i = test_labels[test_filter[:, 0]]\n",
        "  # TODO create latent embeddings for this class\n",
        "  latent_representations = ?\n",
        "  # TODO predict with K means and calculate the mode of these predictions for this class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLUKks1damPi"
      },
      "source": [
        "With that done, we should see that our class seperation isn't the best. We can tell by the fact that our test images when grouped by their labels have the same mode as other classes when K-mean clustered. This means that our seperation of classes in latent space isn't sufficiently large to be a good classifier.\n",
        "\n",
        "To investigate the source of this problem, we can embed an image of each class and then retrieve a set of images that have a similar latent space embedding. Run the following code to view these similar images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Rs0Qeka5jt"
      },
      "outputs": [],
      "source": [
        "# This visualization code expects train_images to be an array of all training images,\n",
        "# and train_latent_representations to be an array of all the corresponding image embeddings.\n",
        "\n",
        "\n",
        "def find_similar_images(images, latent_representations, image_number, K=8):\n",
        "    # Find closest K images using Euclidean distance\n",
        "    query_z = latent_representations[i]\n",
        "    dist = tf.norm(tf.expand_dims(query_z, axis=0) - latent_representations, axis=1)\n",
        "\n",
        "    # Get the indices of the closest images\n",
        "    dist, indices = tf.nn.top_k(-dist, k=K, sorted=True)  # We negate dist to get closest ones\n",
        "\n",
        "    images_to_display = images[indices]\n",
        "\n",
        "    fig, axs = plt.subplots(1, K, figsize=(10, 2))\n",
        "    for example_i in range(8):\n",
        "      axs[example_i].imshow(images_to_display[example_i])\n",
        "    plt.show()\n",
        "\n",
        "train_latent_representations = []\n",
        "for j in range(0, len(train_images), batch_size):\n",
        "  latent_representations.append(encoder.call(train_images[j:j+batch_size]))\n",
        "train_latent_representations = tf.concat(train_latent_representations, axis=0)\n",
        "train_latent_representations = tf.reshape(train_latent_representations, (len(train_latent_representations), -1))\n",
        "\n",
        "for i in range(8):\n",
        "    find_similar_images(train_images, train_latent_representations, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI30S0giuJZx"
      },
      "source": [
        "While our class separation was not high enough to create a good classifier, we can see that images are still being grouped in latent space by their similarity, but images which are close to eachother in the latent aren't always the same class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeSPlmCMu4mG"
      },
      "source": [
        "Conceptual questions TODO:\n",
        "\n",
        "Q1) Suppose that we trained our network using the original autoencoder loss function instead of our denoised loss. The only code change would be:\n",
        "\n",
        "    sum_loss = model.loss_function(predictions, uncorrupted)\n",
        "\n",
        "to\n",
        "\n",
        "    sum_loss = model.loss_function(predictions, corrupted)\n",
        "\n",
        "Do you think our model could still learn to denoise our data? Explain what you think the network would learn in this case. (1-3 sentences)\n",
        "\n",
        "Q2) You will notice that if we used our k-means clustering as a classifier it would perform quite poorly.\n",
        "\n",
        "    Explain why this is occurring (hint: Look at our latent space visualizer) ( 1 - 2 sentences):\n",
        "    \n",
        "    What dataset sanitization step could improve this (hint consider the MINST dataset)? ( 1 - 2 sentences ):\n",
        "\n",
        "Q3): Suppose we have our trained autoencoder and we now sample from the latent space and feed the values into our decoder:\n",
        "\n",
        "    What would you expect to see if we took a mean latent space value for the boat class and passed it to our decoder?( 1 - 2 sentences ):\n",
        "\n",
        "    How would this output change as we ‘walked’ from this value to the mean latent space value of the deer class?( 1 - 2 sentences ):\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk-NLibWuLYO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
