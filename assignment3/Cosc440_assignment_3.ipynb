{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gY_Ma-ke0eY"
      },
      "source": [
        "# COSC440 Assignment 3: Building a Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xeaFoKZe0ea"
      },
      "source": [
        "In the first part of the assignment, we will be introducing and getting familiar with the concept of a convolutional autoencoder. We will use this to denoise images from the CIFAR10 dataset. In the second part we will attempt to apply to this model to explore unsupervised classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HJoXp45e0eb"
      },
      "source": [
        "## Introduction\n",
        "Autoencoders are really cool. They use this architecture of encoder + decoder to learn data representation in an unsupervised manner. Sounds complicated? Let's break it down.\n",
        "\n",
        "\n",
        "![Autoencoders](https://drive.google.com/uc?export=view&id=18iU9_yZfqs2QlTPbaibzkMtdIekLuPj2)\n",
        "\n",
        "As we can see, the autoencoder is divided roughly in two parts: the encoder that encodes the input to a \"code\", or internal representation, that is reduced in dimensions, and the decoder that decodes the \"code\" to an output that is the same dimensions as the input.\n",
        "\n",
        "The autoencoder tries to learn a function h(x)≈x. In other words, it is trying to learn an approximation to the identity function, so as to output x'  that is similar to x. This may sound like a pretty trivial task, why not just use the identity function? Well, it's impossible to find a perfect representation of the identity function since we reduced dimensionality of the input into the code during the encoding part, and try to \"reconstruct\" the original image using only the dimensionally reduced code. E.x. Given an input of a 10x10 image, and a hidden size of 50, the encoder is forced to learn a compressed representation of the image (from 100 units to 50 units). This encoding is almost always lossy, but the point is that we train the encoder to recognize important structures in the input and throw away the unimportant noise.\n",
        "\n",
        "### In this part, we will be building a denoising autoencoder. The input of the autoencoder will be a noisy version of our training data and we will train the model to predict the noise free version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAcauSgEe0ec"
      },
      "source": [
        "## Deconvolution (Transposed-Convolution)\n",
        "Conceptually, deconvolution reverses the effects of convolution. Convolution with a stride of larger than 1 reduces the size of the input, while deconvolution can be described as a \"fractional stride\" convolution, it upscales the size of the input. Here's a visualisation.\n",
        "\n",
        "#### Convolution with stride = 2 *going from bigger to smaller*:\n",
        "![convolution.gif](https://drive.google.com/uc?export=view&id=1TjSgMinNQ629TrdEmcVjKuwWZ9MGMfg7)\n",
        "\n",
        "#### Deconvolution (convolution with fractional stride) *going from smaller to bigger*:\n",
        "![deconvolution.gif](https://drive.google.com/uc?export=view&id=1opzIaiShA5_nLT5Y5Kh_GqPT9IlVeQF5)\n",
        "\n",
        "As we can see, deconvolution upscales the input. This is important for our decoder as its job is to upscale the encoded and compressed internal representation to its original size.\n",
        "Concretely, this is how we implement deconvolution in tensorflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEvAON8ye0ef"
      },
      "source": [
        "``` Python\n",
        "tf.nn.conv2d_transpose(\n",
        "    value, # input tensor: A 4-D Tensor of type float and shape [batch, height, width, in_channels]\n",
        "    filter, # filter tensor: A 4-D Tensor with shape [height, width, output_channels, in_channels]\n",
        "    output_shape, # A 1-D Tensor representing the output shape of the deconvolution op.\n",
        "    strides, # A list of ints. The stride of the sliding window for each dimension of the input tensor.\n",
        "    padding='SAME'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9K0ELkhe0fF"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Enabling the GPU:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "\n",
        "**Note: We are forcing installation of TensorFlow version 2.15.0.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oZGwvU5UsYV",
        "outputId": "a5ef2670-f874-4c56-c3aa-752c43d2f0e3"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHX7FYAJ2RtZ",
        "outputId": "f4e1bf3f-f68e-44c6-f7f7-9be9cbd77dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.0\n",
            "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# # This makes sure that we are using Tensorflow 2\n",
        "# try:\n",
        "#   # Note: %tensorflow_version only exists in Google Colab\n",
        "#   %tensorflow_version 2.x\n",
        "# except Exception:\n",
        "#     pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(1337)  # setting seed for later part of assignment. DO NOT CHANGE\n",
        "\n",
        "print(tf.__version__)\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9fO-Ccn3wRF"
      },
      "source": [
        "If set up correctly, you should see GPU Available: [PhysicalDevice]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHQpPy-tgl3I"
      },
      "source": [
        "### Noise Functions\n",
        "\n",
        "In order to test our autoencoder, we need to add some noise to our CIFAR10 data points. The goal for the autoencoder is to learn the important underlying features (the objects) of the data and to ignore the extra noise.\n",
        "\n",
        "Implement the below different noise functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-qxm7NGloBT",
        "outputId": "4381cb3c-e496-46c1-c857-a95fadc04455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noise functions look good!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This function adds no noise, we use this as a way of comparing the\n",
        "autoencoder when there's no input noise\n",
        "\"\"\"\n",
        "def no_noise(x):\n",
        "  # TODO\n",
        "  return tf.cast(x, tf.float32)\n",
        "\n",
        "\"\"\"\n",
        "This function should add a random uniform tensor of the shape of x between\n",
        "-0.3 to 0.3 to x.\n",
        "\n",
        "It should then \"clip\" x to between 0 and 1 (hint: check out tf.clip_by_value)\n",
        "\"\"\"\n",
        "def random_noise(x):\n",
        "  # TODO\n",
        "  x = tf.cast(x, tf.float32)\n",
        "  noise = tf.Variable(tf.random.uniform(x.shape,\n",
        "                minval=-0.3,\n",
        "                maxval=0.3,\n",
        "                dtype=tf.float32),\n",
        "                name = \"noise\")\n",
        "  return tf.clip_by_value(x + noise, 0, 1)\n",
        "\n",
        "\"\"\"\n",
        "This function should multiply a random uniform tensor of the shape of x between\n",
        "0 to 2.0 to x.\n",
        "\n",
        "It should then \"clip\" x to between 0 and 1 (hint: check out tf.clip_by_value)\n",
        "\"\"\"\n",
        "def random_scale(x):\n",
        "  # TODO\n",
        "  x = tf.cast(x, tf.float32)\n",
        "  noise = tf.Variable(tf.random.uniform(x.shape,\n",
        "                minval=0,\n",
        "                maxval=2.0,\n",
        "                dtype=tf.float32),\n",
        "                name = \"noise\")\n",
        "  return tf.clip_by_value(x + noise, 0, 1)\n",
        "\n",
        "# some \"unit tests\"\n",
        "x = [[0.3,0.1],[0.2,0]]\n",
        "y = [[0.1,1],[0,0.33]]\n",
        "result_1 = random_noise(x)\n",
        "result_2 = random_scale(x)\n",
        "result_3 = random_noise(y)\n",
        "result_4 = random_scale(y)\n",
        "\n",
        "for res in [result_1, result_2, result_3, result_4]:\n",
        "  assert(res.shape == (2,2))\n",
        "  assert(res.dtype == tf.float32)\n",
        "  assert(np.max(np.array(res)) <= 1.0)\n",
        "  assert(np.min(np.array(res)) >= 0.0)\n",
        "\n",
        "print(\"Noise functions look good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWuS-lvYe0fU"
      },
      "source": [
        "## Architecture Definition\n",
        "We want to construct the following architecture:\n",
        "\n",
        "(Due to the limitations of the rendering engine I'm using, there is a small inaccuracy: the connections in the decoder *{the red prisms}* should be upscaling, not downscaling like in the encoder. - original creator of lab)\n",
        "\n",
        "Shapes:\n",
        "\n",
        "Layer | Shape of Output\n",
        "--- | ---\n",
        "Input | (batch_size, 32, 32, 3)\n",
        "encoder_conv_1    | (batch_size, 16, 16, 8)\n",
        "encoder_conv_2    | (batch_size, 8, 8, 16)  \n",
        "encoder_conv_3    | (batch_size, 4, 4, 32)  \n",
        "decoder_deconv_1  | (batch_size, 8, 8, 16)  \n",
        "decoder_deconv_2  | (batch_size, 16, 16, 8)\n",
        "decoder_deconv_3  | (batch_size, 32, 32, 3)\n",
        "\n",
        "![Architecture](https://drive.usercontent.google.com/download?id=1-jZFte-UE-L-lFiTHlAhgFyDDYPXRVvT&export=view)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp9a4duA4_Z-"
      },
      "source": [
        "### **Encoder**\n",
        "Let's first build the encoding portion of our autoencoder. We're going to directly use Keras layers to help us out by subclassing the Layer class. See this for more information on subclassing Layers: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
        "\n",
        "This should have 3 convolution layers. The layers should be initialized with random normal with standard deviation of 0.1. Also, please use an activation layer of leaky_relu with alpha=0.2. Use a kernel size of 3 and padding of same. Think about what the filter count and stride should be by looking at the output shapes of the architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "VjlwlP5R5EZY"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "       super(Encoder, self).__init__()\n",
        "       self.encoder_conv_1 = tf.keras.layers.Conv2D(\n",
        "           filters=8,\n",
        "           kernel_size=3,\n",
        "           strides=2,\n",
        "           padding='same',\n",
        "           kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
        "           activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "       )\n",
        "       self.encoder_conv_2 = tf.keras.layers.Conv2D(\n",
        "           filters=16,\n",
        "           kernel_size=3,\n",
        "           strides=2,\n",
        "           padding='same',\n",
        "           kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
        "           activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "       )\n",
        "       self.encoder_conv_3 = tf.keras.layers.Conv2D(\n",
        "           filters=32,\n",
        "           kernel_size=3,\n",
        "           strides=2,\n",
        "           padding='same',\n",
        "           kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
        "           activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "       )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, images):\n",
        "      x = self.encoder_conv_1(images)\n",
        "      x = self.encoder_conv_2(x)\n",
        "      x = self.encoder_conv_3(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVQgB6sg9cwP"
      },
      "source": [
        "### **Decoder**\n",
        "Now let's build the decoder portion of our autoencoder.\n",
        "\n",
        "This should have 3 deconvolution layers. For the first deconvolution layer please use tf.nn.conv2d_transpose to implement the deconvolution. The other two may be keras layers (Conv2DTranspose). Use the same kernel / activation /initialization as the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "QbVcfiq29hIs"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.filters = tf.Variable(tf.random.normal([3, 3, 16, 32], stddev=0.1))\n",
        "        self.decoder_deconv_1 = lambda x: tf.nn.conv2d_transpose(\n",
        "            input = x,                    \n",
        "            filters = self.filters,\n",
        "            output_shape = [tf.shape(x)[0],8,8,16]\n",
        "            strides = [1,2,2,1], \n",
        "            padding='SAME')\n",
        "        self.decoder_deconv_2 = tf.keras.layers.Conv2DTranspose(\n",
        "            filters=8,\n",
        "            kernel_size=3,\n",
        "            strides=2,\n",
        "            padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
        "            activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        )\n",
        "        self.decoder_deconv_3 = tf.keras.layers.Conv2DTranspose(\n",
        "            filters=3,\n",
        "            kernel_size=3,\n",
        "            strides=2,\n",
        "            padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
        "            activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        )\n",
        "    @tf.function\n",
        "    def call(self, encoder_output):\n",
        "      x = self.decoder_deconv_1(encoder_output)\n",
        "      x = self.decoder_deconv_2(x)\n",
        "      x = self.decoder_deconv_3(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XNWonQwEX3S"
      },
      "source": [
        "Now let's combine everything to create our autoencoder.\n",
        "\n",
        "Fill in the call and loss_function.\n",
        "\n",
        "Our loss equation is:\n",
        "$$\\sum (y - x)^2$$\n",
        "\n",
        "\n",
        "y: the encoded image\n",
        "\n",
        "x: the original corrupted image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "gtD2UkkLEcIF"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, images):\n",
        "      encoded = self.encoder(images)\n",
        "      decoded = self.decoder(encoded)\n",
        "      return decoded\n",
        "\n",
        "    @tf.function\n",
        "    def loss_function(self, encoded, originals):\n",
        "      encoded = tf.dtypes.cast(encoded, tf.float32)\n",
        "      originals = tf.dtypes.cast(originals, tf.float32)\n",
        "      return tf.reduce_sum(tf.square(originals - encoded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8PdssiPqQi8"
      },
      "source": [
        "### Train Loop\n",
        "Next, please fill in the train loop for training.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gUUT4loWtllD"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, images, noise_function):\n",
        "  corrupted = noise_function(images)\n",
        "  uncorrupted = images\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(corrupted)\n",
        "    loss = model.loss_function(predictions, uncorrupted)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOaCajFzqZQO"
      },
      "source": [
        "### Total Loss\n",
        "This function that calculates the total loss over the entire data set,\n",
        "we use this to track the loss over each epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "gxVP3tDatlvQ"
      },
      "outputs": [],
      "source": [
        "def total_loss(model, images, noise_function):\n",
        "  corrupted = noise_function(images)\n",
        "  uncorrupted = images\n",
        "  predictions = model(corrupted)\n",
        "  sum_loss = model.loss_function(predictions, uncorrupted)\n",
        "  return sum_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYgBuoPAmvRa"
      },
      "source": [
        "## Test your autoencoder\n",
        "\n",
        "Run the following code to run your autoencoder. You should see it print out images and the total loss of the autoencoder each epoch.\n",
        "\n",
        "Be aware that the loss generally should go down each epoch, but might not always. Use your judgement and check with the TA after this runs.\n",
        "\n",
        "It probably will take around 5 minutes (make sure you are using a GPU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "GPBJIxfsq1fG",
        "outputId": "21edfb08-d993-46bc-9cca-1f1686ee54e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Showing autoencoder for noise function: <function random_noise at 0x000002218611AD38>\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    C:\\Users\\69590\\AppData\\Local\\Temp\\ipykernel_8088\\2955778032.py:10 call  *\n        decoded = self.decoder(encoded)\n    C:\\Users\\69590\\AppData\\Local\\Temp\\ipykernel_8088\\1963758583.py:28 call  *\n        x = self.decoder_deconv_1(encoder_output)\n    C:\\Users\\69590\\AppData\\Local\\Temp\\ipykernel_8088\\1963758583.py:9 None  *\n        strides = 2, # A list of ints. The stride of the sliding window for each dimension of the input tensor.\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:262 __call__  **\n        return cls._variable_v2_call(*args, **kwargs)\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:256 _variable_v2_call\n        shape=shape)\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:731 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8088\\1758485694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m       \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8088\\3411808820.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, images, noise_function)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrupted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muncorrupted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3885\u001b[0m     \u001b[1;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m     \u001b[1;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\69590\\AppData\\Local\\Temp\\ipykernel_8088\\2955778032.py:10 call  *\n        decoded = self.decoder(encoded)\n    C:\\Users\\69590\\AppData\\Local\\Temp\\ipykernel_8088\\1963758583.py:28 call  *\n        x = self.decoder_deconv_1(encoder_output)\n    C:\\Users\\69590\\AppData\\Local\\Temp\\ipykernel_8088\\1963758583.py:9 None  *\n        strides = 2, # A list of ints. The stride of the sliding window for each dimension of the input tensor.\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:262 __call__  **\n        return cls._variable_v2_call(*args, **kwargs)\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:256 _variable_v2_call\n        shape=shape)\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    d:\\anaconda3\\envs\\yzsnail\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:731 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n"
          ]
        }
      ],
      "source": [
        "# This code is helper code to plot the cifar10 images so you\n",
        "# can see your autoencoder in action!\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_examples = 10\n",
        "batch_size = 100\n",
        "n_epochs = 25\n",
        "\n",
        "\n",
        "(train_images, _), (test_images, _) = tf.keras.datasets.cifar10.load_data()\n",
        "test_images = test_images / 255\n",
        "example_images = test_images[:n_examples]\n",
        "train_images = train_images / 255\n",
        "\n",
        "\n",
        "def showImages(model, noise_function):\n",
        "  corrupted = noise_function(example_images)\n",
        "  recon = tf.clip_by_value(model(corrupted), 0, 1)\n",
        "\n",
        "  fig, axs = plt.subplots(2, n_examples, figsize=(10, 2))\n",
        "  for example_i in range(n_examples):\n",
        "      axs[0][example_i].imshow(corrupted[example_i])\n",
        "      axs[1][example_i].imshow(recon[example_i])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Runs the autoencoder\n",
        "# We'll just be testing the random_noise function for training\n",
        "for noise_function in [random_noise]:\n",
        "  print(\"Showing autoencoder for noise function: {0}\".format(noise_function))\n",
        "  model = AutoEncoder()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "  for i in range(n_epochs):\n",
        "    for j in range(0, len(train_images), batch_size):\n",
        "      train(model, optimizer, train_images[j:j+batch_size], noise_function)\n",
        "\n",
        "    print(\"Epoch: \", i)\n",
        "    sum_loss = total_loss(model, test_images, noise_function)\n",
        "    print(\"Total Loss: {0}\".format(sum_loss))\n",
        "    showImages(model, noise_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sBNgo9Ze0fy"
      },
      "source": [
        "## Checkpoint!\n",
        "\n",
        "Check to see if your denoiser works! If it works, you should see the autoencoder images remove noise from the original inputs. The top row for each epoch is the input image and the bottom is the autoencoded image.\n",
        "\n",
        "If you wish to you can check out how the different noise functions are handled by the autoencoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6dyRrpWe0fz"
      },
      "source": [
        "## Visualizing the Data Representation\n",
        "\n",
        "We can now visualize where CIFAR10 data points are positioned in the latent space. Run the visualization code below to open Tensorboard. On the bottom right you can switch visualiztion to T-SNE or PCA for a 3D representation.\n",
        "\n",
        "Note: You may have to run the cell twice for it to actually render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XYQAxZt048Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "NUM_SAMPLES = 1000\n",
        "\n",
        "(train_images, train_labels),(test_images,test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "test_images = test_images / 255\n",
        "train_images = train_images / 255\n",
        "\n",
        "z_test = model.encoder(tf.reshape(test_images, [-1, 32, 32, 3]))\n",
        "z_test = np.reshape(z_test, [len(test_images), -1])\n",
        "\n",
        "z_train = model.encoder(tf.reshape(train_images, [-1, 32, 32, 3]))\n",
        "z_train = np.reshape(z_train, [len(train_images), -1])\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "log_dir = \"tensorboard/\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "total_images = len(test_images)\n",
        "\n",
        "z_test_tensor = torch.tensor(z_test[:total_images])\n",
        "test_labels_tensor = [int(test_labels[i]) for i in range(total_images)]\n",
        "test_images_tensor = torch.tensor(test_images[:total_images]).permute(0, 3, 1, 2)\n",
        "\n",
        "writer.add_embedding(z_test_tensor, metadata=test_labels_tensor, label_img=test_images_tensor)\n",
        "\n",
        "%tensorboard --logdir {log_dir}\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQgDnxHCW4Md"
      },
      "source": [
        "Now that we have a trained version of the auto-encoder we want to take the output of just the encoder and perform k-mean clustering on it. Fill out the code below and perform k-mean clustering on the latent space of the training examples.\n",
        "\n",
        "Note: Running k-means should take about 2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Aq7gMxQX0b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 10\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10, max_iter=1000)\n",
        "encoder = model.encoder\n",
        "# TODO create latent space embeddings of training images\n",
        "latent_representations = ?\n",
        "assert(tf.is_tensor(latent_representations))\n",
        "assert(latent_representations.shape[0] == 50000)\n",
        "assert(latent_representations.shape[0] == 512)\n",
        "print('latent_representations is the right shape and type')\n",
        "\n",
        "# TODO perform kmeans clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0q18pevslGP"
      },
      "source": [
        "Lets check our class seperations now. In the next cell, group the test images by class, calculate their latent space embeddings, and run kmeans.predict on them all.\n",
        "\n",
        "Use this to calculate the mode of the predictions for each of the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MH4BDj5t_dl"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "for i in range(NUM_CLASSES):\n",
        "  test_filter = test_labels == i\n",
        "  test_images_class_i = test_images[test_filter[:, 0]]\n",
        "  test_labels_class_i = test_labels[test_filter[:, 0]]\n",
        "  # TODO create latent embeddings for this class\n",
        "  latent_representations = ?\n",
        "  # TODO predict with K means and calculate the mode of these predictions for this class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLUKks1damPi"
      },
      "source": [
        "With that done, we should see that our class seperation isn't the best. We can tell by the fact that our test images when grouped by their labels have the same mode as other classes when K-mean clustered. This means that our seperation of classes in latent space isn't sufficiently large to be a good classifier.\n",
        "\n",
        "To investigate the source of this problem, we can embed an image of each class and then retrieve a set of images that have a similar latent space embedding. Run the following code to view these similar images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Rs0Qeka5jt"
      },
      "outputs": [],
      "source": [
        "# This visualization code expects train_images to be an array of all training images,\n",
        "# and train_latent_representations to be an array of all the corresponding image embeddings.\n",
        "\n",
        "\n",
        "def find_similar_images(images, latent_representations, image_number, K=8):\n",
        "    # Find closest K images using Euclidean distance\n",
        "    query_z = latent_representations[i]\n",
        "    dist = tf.norm(tf.expand_dims(query_z, axis=0) - latent_representations, axis=1)\n",
        "\n",
        "    # Get the indices of the closest images\n",
        "    dist, indices = tf.nn.top_k(-dist, k=K, sorted=True)  # We negate dist to get closest ones\n",
        "\n",
        "    images_to_display = images[indices]\n",
        "\n",
        "    fig, axs = plt.subplots(1, K, figsize=(10, 2))\n",
        "    for example_i in range(8):\n",
        "      axs[example_i].imshow(images_to_display[example_i])\n",
        "    plt.show()\n",
        "\n",
        "train_latent_representations = []\n",
        "for j in range(0, len(train_images), batch_size):\n",
        "  latent_representations.append(encoder.call(train_images[j:j+batch_size]))\n",
        "train_latent_representations = tf.concat(train_latent_representations, axis=0)\n",
        "train_latent_representations = tf.reshape(train_latent_representations, (len(train_latent_representations), -1))\n",
        "\n",
        "for i in range(8):\n",
        "    find_similar_images(train_images, train_latent_representations, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI30S0giuJZx"
      },
      "source": [
        "While our class separation was not high enough to create a good classifier, we can see that images are still being grouped in latent space by their similarity, but images which are close to eachother in the latent aren't always the same class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeSPlmCMu4mG"
      },
      "source": [
        "Conceptual questions TODO:\n",
        "\n",
        "Q1) Suppose that we trained our network using the original autoencoder loss function instead of our denoised loss. The only code change would be:\n",
        "\n",
        "    sum_loss = model.loss_function(predictions, uncorrupted)\n",
        "\n",
        "to\n",
        "\n",
        "    sum_loss = model.loss_function(predictions, corrupted)\n",
        "\n",
        "Do you think our model could still learn to denoise our data? Explain what you think the network would learn in this case. (1-3 sentences)\n",
        "\n",
        "Q2) You will notice that if we used our k-means clustering as a classifier it would perform quite poorly.\n",
        "\n",
        "    Explain why this is occurring (hint: Look at our latent space visualizer) ( 1 - 2 sentences):\n",
        "    \n",
        "    What dataset sanitization step could improve this (hint consider the MINST dataset)? ( 1 - 2 sentences ):\n",
        "\n",
        "Q3): Suppose we have our trained autoencoder and we now sample from the latent space and feed the values into our decoder:\n",
        "\n",
        "    What would you expect to see if we took a mean latent space value for the boat class and passed it to our decoder?( 1 - 2 sentences ):\n",
        "\n",
        "    How would this output change as we ‘walked’ from this value to the mean latent space value of the deer class?( 1 - 2 sentences ):\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk-NLibWuLYO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
